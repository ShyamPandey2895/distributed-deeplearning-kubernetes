{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/StefanoFiora/.virtualenvs/deep-learning/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x10f500e48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler_env = os.environ.copy()\n",
    "# Assign env variables to run the scheduler node.\n",
    "scheduler_env.update({\n",
    "    \"DMLC_ROLE\": \"scheduler\",  # this is the role of the node (scheduler | server | worker)\n",
    "    \"DMLC_PS_ROOT_PORT\": \"9000\",\n",
    "    \"DMLC_PS_ROOT_URI\": \"127.0.0.1\",\n",
    "    # These two are needed because every node needs to know about all the others.\n",
    "    \"DMLC_NUM_SERVER\": \"1\",\n",
    "    \"DMLC_NUM_WORKER\": \"1\",\n",
    "    \"PS_VERBOSE\": \"2\"\n",
    "})\n",
    "# Run the scheduler as a subprocess\n",
    "subprocess.Popen(\"python -c 'import mxnet'\", shell=True, env=scheduler_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x10f509940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign env variables to run the scheduler node.\n",
    "server_env = os.environ.copy()\n",
    "server_env.update({\n",
    "    \"DMLC_ROLE\": \"server\",\n",
    "    # Same IP and PORT as before, becuase the server auto assign itself a free port.\n",
    "    # These are needed so that the sever can communicate with the schedule to discover\n",
    "    # the other cluster's nodes.\n",
    "    \"DMLC_PS_ROOT_PORT\": \"9000\",\n",
    "    \"DMLC_PS_ROOT_URI\": \"127.0.0.1\",\n",
    "    \"DMLC_NUM_SERVER\": \"1\",\n",
    "    \"DMLC_NUM_WORKER\": \"1\",\n",
    "    \"PS_VERBOSE\": \"2\"\n",
    "})\n",
    "# Run the scheduler as a subprocess\n",
    "subprocess.Popen(\"python -c 'import mxnet'\", shell=True, env=server_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign env variables to run the scheduler node.\n",
    "os.environ.update({\n",
    "    \"DMLC_ROLE\": \"worker\",\n",
    "    \"DMLC_PS_ROOT_PORT\": \"9000\",\n",
    "    \"DMLC_PS_ROOT_URI\": \"127.0.0.1\",\n",
    "    \"DMLC_NUM_SERVER\": \"1\",\n",
    "    \"DMLC_NUM_WORKER\": \"1\",\n",
    "    \"PS_VERBOSE\": \"2\"\n",
    "})\n",
    "worker_env = os.environ.copy()\n",
    "kv_store = mxnet.kv.create('dist_async')\n",
    "# Run the scheduler as a subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [20]\tSpeed: 1204.15 samples/sec\tmse=0.931558\n",
      "INFO:root:Epoch[0] Batch [40]\tSpeed: 828.68 samples/sec\tmse=0.275269\n",
      "INFO:root:Epoch[0] Batch [60]\tSpeed: 1118.00 samples/sec\tmse=0.136297\n",
      "INFO:root:Epoch[0] Batch [80]\tSpeed: 976.33 samples/sec\tmse=0.098951\n",
      "INFO:root:Epoch[0] Train-mse=0.083322\n",
      "INFO:root:Epoch[0] Time cost=0.112\n",
      "INFO:root:Epoch[0] Validation-mse=46.740801\n",
      "INFO:root:Epoch[1] Batch [20]\tSpeed: 952.15 samples/sec\tmse=0.047918\n",
      "INFO:root:Epoch[1] Batch [40]\tSpeed: 1113.11 samples/sec\tmse=0.056701\n",
      "INFO:root:Epoch[1] Batch [60]\tSpeed: 1090.28 samples/sec\tmse=0.038048\n",
      "INFO:root:Epoch[1] Batch [80]\tSpeed: 1463.70 samples/sec\tmse=0.033006\n",
      "INFO:root:Epoch[1] Train-mse=0.026844\n",
      "INFO:root:Epoch[1] Time cost=0.095\n",
      "INFO:root:Epoch[1] Validation-mse=15.709116\n",
      "INFO:root:Epoch[2] Batch [20]\tSpeed: 1270.15 samples/sec\tmse=0.015446\n",
      "INFO:root:Epoch[2] Batch [40]\tSpeed: 1587.07 samples/sec\tmse=0.018235\n",
      "INFO:root:Epoch[2] Batch [60]\tSpeed: 1371.83 samples/sec\tmse=0.012107\n",
      "INFO:root:Epoch[2] Batch [80]\tSpeed: 1543.36 samples/sec\tmse=0.010350\n",
      "INFO:root:Epoch[2] Train-mse=0.008610\n",
      "INFO:root:Epoch[2] Time cost=0.077\n",
      "INFO:root:Epoch[2] Validation-mse=5.235878\n",
      "INFO:root:Epoch[3] Batch [20]\tSpeed: 1096.78 samples/sec\tmse=0.004990\n",
      "INFO:root:Epoch[3] Batch [40]\tSpeed: 1686.05 samples/sec\tmse=0.005900\n",
      "INFO:root:Epoch[3] Batch [60]\tSpeed: 1392.88 samples/sec\tmse=0.003874\n",
      "INFO:root:Epoch[3] Batch [80]\tSpeed: 1570.25 samples/sec\tmse=0.003277\n",
      "INFO:root:Epoch[3] Train-mse=0.002772\n",
      "INFO:root:Epoch[3] Time cost=0.077\n",
      "INFO:root:Epoch[3] Validation-mse=1.732853\n",
      "INFO:root:Epoch[4] Batch [20]\tSpeed: 1137.21 samples/sec\tmse=0.001617\n",
      "INFO:root:Epoch[4] Batch [40]\tSpeed: 1590.32 samples/sec\tmse=0.001913\n",
      "INFO:root:Epoch[4] Batch [60]\tSpeed: 1441.04 samples/sec\tmse=0.001245\n",
      "INFO:root:Epoch[4] Batch [80]\tSpeed: 1493.30 samples/sec\tmse=0.001045\n",
      "INFO:root:Epoch[4] Train-mse=0.000895\n",
      "INFO:root:Epoch[4] Time cost=0.085\n",
      "INFO:root:Epoch[4] Validation-mse=0.570462\n",
      "INFO:root:Epoch[5] Batch [20]\tSpeed: 1354.64 samples/sec\tmse=0.000525\n",
      "INFO:root:Epoch[5] Batch [40]\tSpeed: 1340.29 samples/sec\tmse=0.000621\n",
      "INFO:root:Epoch[5] Batch [60]\tSpeed: 1505.25 samples/sec\tmse=0.000401\n",
      "INFO:root:Epoch[5] Batch [80]\tSpeed: 1137.72 samples/sec\tmse=0.000335\n",
      "INFO:root:Epoch[5] Train-mse=0.000289\n",
      "INFO:root:Epoch[5] Time cost=0.083\n",
      "INFO:root:Epoch[5] Validation-mse=0.187049\n",
      "INFO:root:Epoch[6] Batch [20]\tSpeed: 1120.44 samples/sec\tmse=0.000170\n",
      "INFO:root:Epoch[6] Batch [40]\tSpeed: 1053.96 samples/sec\tmse=0.000201\n",
      "INFO:root:Epoch[6] Batch [60]\tSpeed: 1172.31 samples/sec\tmse=0.000130\n",
      "INFO:root:Epoch[6] Batch [80]\tSpeed: 994.98 samples/sec\tmse=0.000108\n",
      "INFO:root:Epoch[6] Train-mse=0.000094\n",
      "INFO:root:Epoch[6] Time cost=0.102\n",
      "INFO:root:Epoch[6] Validation-mse=0.061147\n",
      "INFO:root:Epoch[7] Batch [20]\tSpeed: 1540.58 samples/sec\tmse=0.000055\n",
      "INFO:root:Epoch[7] Batch [40]\tSpeed: 1032.43 samples/sec\tmse=0.000065\n",
      "INFO:root:Epoch[7] Batch [60]\tSpeed: 1227.00 samples/sec\tmse=0.000042\n",
      "INFO:root:Epoch[7] Batch [80]\tSpeed: 1213.68 samples/sec\tmse=0.000035\n",
      "INFO:root:Epoch[7] Train-mse=0.000030\n",
      "INFO:root:Epoch[7] Time cost=0.092\n",
      "INFO:root:Epoch[7] Validation-mse=0.019945\n",
      "INFO:root:Epoch[8] Batch [20]\tSpeed: 1298.97 samples/sec\tmse=0.000018\n",
      "INFO:root:Epoch[8] Batch [40]\tSpeed: 1302.42 samples/sec\tmse=0.000021\n",
      "INFO:root:Epoch[8] Batch [60]\tSpeed: 1196.59 samples/sec\tmse=0.000014\n",
      "INFO:root:Epoch[8] Batch [80]\tSpeed: 922.34 samples/sec\tmse=0.000011\n",
      "INFO:root:Epoch[8] Train-mse=0.000010\n",
      "INFO:root:Epoch[8] Time cost=0.087\n",
      "INFO:root:Epoch[8] Validation-mse=0.006495\n",
      "INFO:root:Epoch[9] Batch [20]\tSpeed: 1226.98 samples/sec\tmse=0.000006\n",
      "INFO:root:Epoch[9] Batch [40]\tSpeed: 1180.70 samples/sec\tmse=0.000007\n",
      "INFO:root:Epoch[9] Batch [60]\tSpeed: 1097.02 samples/sec\tmse=0.000004\n",
      "INFO:root:Epoch[9] Batch [80]\tSpeed: 855.58 samples/sec\tmse=0.000004\n",
      "INFO:root:Epoch[9] Train-mse=0.000003\n",
      "INFO:root:Epoch[9] Time cost=0.101\n",
      "INFO:root:Epoch[9] Validation-mse=0.002112\n",
      "INFO:root:Epoch[10] Batch [20]\tSpeed: 1533.40 samples/sec\tmse=0.000002\n",
      "INFO:root:Epoch[10] Batch [40]\tSpeed: 996.99 samples/sec\tmse=0.000002\n",
      "INFO:root:Epoch[10] Batch [60]\tSpeed: 1267.10 samples/sec\tmse=0.000001\n",
      "INFO:root:Epoch[10] Batch [80]\tSpeed: 808.50 samples/sec\tmse=0.000001\n",
      "INFO:root:Epoch[10] Train-mse=0.000001\n",
      "INFO:root:Epoch[10] Time cost=0.105\n",
      "INFO:root:Epoch[10] Validation-mse=0.000686\n",
      "INFO:root:Epoch[11] Batch [20]\tSpeed: 1145.16 samples/sec\tmse=0.000001\n",
      "INFO:root:Epoch[11] Batch [40]\tSpeed: 1193.53 samples/sec\tmse=0.000001\n",
      "INFO:root:Epoch[11] Batch [60]\tSpeed: 969.59 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[11] Batch [80]\tSpeed: 877.00 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[11] Train-mse=0.000000\n",
      "INFO:root:Epoch[11] Time cost=0.099\n",
      "INFO:root:Epoch[11] Validation-mse=0.000223\n",
      "INFO:root:Epoch[12] Batch [20]\tSpeed: 1889.79 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[12] Batch [40]\tSpeed: 854.70 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[12] Batch [60]\tSpeed: 1544.27 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[12] Batch [80]\tSpeed: 855.28 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[12] Train-mse=0.000000\n",
      "INFO:root:Epoch[12] Time cost=0.098\n",
      "INFO:root:Epoch[12] Validation-mse=0.000072\n",
      "INFO:root:Epoch[13] Batch [20]\tSpeed: 1415.01 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[13] Batch [40]\tSpeed: 1273.01 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[13] Batch [60]\tSpeed: 1507.39 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[13] Batch [80]\tSpeed: 1176.90 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[13] Train-mse=0.000000\n",
      "INFO:root:Epoch[13] Time cost=0.081\n",
      "INFO:root:Epoch[13] Validation-mse=0.000023\n",
      "INFO:root:Epoch[14] Batch [20]\tSpeed: 1437.29 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[14] Batch [40]\tSpeed: 1593.40 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[14] Batch [60]\tSpeed: 968.85 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[14] Batch [80]\tSpeed: 1081.62 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[14] Train-mse=0.000000\n",
      "INFO:root:Epoch[14] Time cost=0.091\n",
      "INFO:root:Epoch[14] Validation-mse=0.000008\n",
      "INFO:root:Epoch[15] Batch [20]\tSpeed: 1194.46 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[15] Batch [40]\tSpeed: 836.95 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[15] Batch [60]\tSpeed: 1582.91 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[15] Batch [80]\tSpeed: 1531.63 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[15] Train-mse=0.000000\n",
      "INFO:root:Epoch[15] Time cost=0.091\n",
      "INFO:root:Epoch[15] Validation-mse=0.000002\n",
      "INFO:root:Epoch[16] Batch [20]\tSpeed: 1332.86 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[16] Batch [40]\tSpeed: 1551.46 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[16] Batch [60]\tSpeed: 1219.66 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[16] Batch [80]\tSpeed: 1651.37 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[16] Train-mse=0.000000\n",
      "INFO:root:Epoch[16] Time cost=0.084\n",
      "INFO:root:Epoch[16] Validation-mse=0.000001\n",
      "INFO:root:Epoch[17] Batch [20]\tSpeed: 2083.35 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[17] Batch [40]\tSpeed: 879.16 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[17] Batch [60]\tSpeed: 2273.83 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[17] Batch [80]\tSpeed: 1629.46 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[17] Train-mse=0.000000\n",
      "INFO:root:Epoch[17] Time cost=0.078\n",
      "INFO:root:Epoch[17] Validation-mse=0.000000\n",
      "INFO:root:Epoch[18] Batch [20]\tSpeed: 1768.96 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[18] Batch [40]\tSpeed: 1030.93 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[18] Batch [60]\tSpeed: 1403.88 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[18] Batch [80]\tSpeed: 1183.78 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[18] Train-mse=0.000000\n",
      "INFO:root:Epoch[18] Time cost=0.084\n",
      "INFO:root:Epoch[18] Validation-mse=0.000000\n",
      "INFO:root:Epoch[19] Batch [20]\tSpeed: 2111.99 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[19] Batch [40]\tSpeed: 1676.45 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[19] Batch [60]\tSpeed: 1352.54 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[19] Batch [80]\tSpeed: 1387.95 samples/sec\tmse=0.000000\n",
      "INFO:root:Epoch[19] Train-mse=0.000000\n",
      "INFO:root:Epoch[19] Time cost=0.075\n",
      "INFO:root:Epoch[19] Validation-mse=0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict using trained model\n",
      "Eval moel using MSE score function\n",
      "Achieved 0.000000 validation MSE\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed\n",
    "mx.random.seed(42)\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "#Training data\n",
    "train_data = np.random.uniform(0, 1, [100, 2])\n",
    "train_label = np.array([train_data[i][0] + 2 * train_data[i][1] for i in range(100)])\n",
    "batch_size = 1\n",
    "\n",
    "#Evaluation Data\n",
    "eval_data = np.array([[7,2],[6,10],[12,2]])\n",
    "eval_label = np.array([11,26,16])\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(train_data, train_label, batch_size, shuffle=True, label_name='lin_reg_label')\n",
    "eval_iter = mx.io.NDArrayIter(eval_data, eval_label, batch_size, shuffle=False, label_name='lin_reg_label')\n",
    "\n",
    "X = mx.sym.Variable('data')\n",
    "Y = mx.symbol.Variable('lin_reg_label')\n",
    "fully_connected_layer  = mx.sym.FullyConnected(data=X, name='fc1', num_hidden = 1)\n",
    "lro = mx.sym.LinearRegressionOutput(data=fully_connected_layer, label=Y, name=\"lro\")\n",
    "\n",
    "model = mx.mod.Module(\n",
    "    symbol = lro ,\n",
    "    data_names=['data'],\n",
    "    label_names = ['lin_reg_label']# network structure\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(train_iter, eval_iter,\n",
    "            optimizer_params={'learning_rate':0.01, 'momentum': 0.9},\n",
    "            num_epoch=20,\n",
    "            eval_metric='mse',\n",
    "            batch_end_callback = mx.callback.Speedometer(batch_size, frequent=20))  # log every `frequent` batches\n",
    "\n",
    "print(\"Predict using trained model\" )\n",
    "model.predict(eval_iter).asnumpy()\n",
    "\n",
    "print(\"Eval moel using MSE score function\")\n",
    "metric = mx.metric.MSE()\n",
    "mse = model.score(eval_iter, metric)\n",
    "print(\"Achieved {0:.6f} validation MSE\".format(mse[0][1]))\n",
    "assert model.score(eval_iter, metric)[0][1] < 0.01001, \"Achieved MSE (%f) is larger than expected (0.01001)\" % model.score(eval_iter, metric)[0][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
